wandb: Currently logged in as: dahoas. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.2
wandb: Run data is saved locally in /fsx/alex/repos/trl/wandb/run-20220915_203348-12ipiqfk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-totem-21
wandb: ‚≠êÔ∏è View project at https://wandb.ai/dahoas/trl_accelerate
wandb: üöÄ View run at https://wandb.ai/dahoas/trl_accelerate/runs/12ipiqfk
Reusing dataset imdb (/home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)
Loading cached processed dataset at /home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-e68378636d846987.arrow
/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated, use `top_k=1` if you want similar functionnality
  warnings.warn(
Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at lvwerra/gpt2-imdb and are newly initialized: ['transformer.h.9.attn.masked_bias', 'v_head.summary.weight', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'v_head.summary.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at lvwerra/gpt2-imdb and are newly initialized: ['transformer.h.9.attn.masked_bias', 'v_head.summary.weight', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'v_head.summary.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-40610166598a103e.arrow
DEVICE:  cuda
NUM EPOCHS:  313
0it [00:00, ?it/s]1it [00:16, 16.89s/it]2it [00:33, 16.83s/it]3it [00:50, 17.04s/it]4it [01:07, 16.94s/it]5it [01:24, 16.76s/it]6it [01:41, 16.78s/it]7it [01:57, 16.71s/it]8it [02:18, 18.02s/it]/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/pipelines/base.py:1036: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(
9it [02:42, 19.90s/it]10it [03:09, 22.22s/it]11it [03:26, 20.58s/it]12it [03:43, 19.57s/it]13it [03:59, 18.48s/it]14it [04:16, 17.92s/it]15it [04:33, 17.52s/it]16it [04:49, 17.20s/it]17it [05:06, 17.01s/it]18it [05:23, 17.05s/it]19it [05:41, 17.29s/it]20it [05:58, 17.36s/it]21it [06:16, 17.55s/it]22it [06:34, 17.58s/it]23it [06:51, 17.53s/it]24it [07:09, 17.66s/it]25it [07:34, 19.74s/it]26it [07:50, 18.72s/it]27it [08:07, 18.05s/it]28it [08:32, 20.18s/it]29it [08:58, 21.88s/it]30it [09:21, 22.31s/it]31it [09:47, 23.39s/it]32it [10:09, 23.10s/it]33it [10:27, 21.41s/it]34it [10:43, 19.96s/it]35it [11:00, 18.87s/it]36it [11:17, 18.30s/it]37it [11:34, 17.89s/it]38it [11:50, 17.46s/it]39it [12:07, 17.36s/it]40it [12:24, 17.31s/it]41it [12:42, 17.39s/it]42it [13:00, 17.45s/it]43it [13:31, 21.80s/it]44it [14:06, 25.54s/it]45it [14:41, 28.53s/it]46it [15:17, 30.67s/it]47it [15:50, 31.34s/it]48it [16:23, 31.83s/it]49it [16:57, 32.57s/it]50it [17:32, 33.20s/it]51it [18:07, 33.87s/it]52it [18:43, 34.40s/it]53it [19:18, 34.62s/it]54it [19:53, 34.77s/it]55it [20:31, 35.80s/it]56it [21:07, 35.81s/it]57it [21:38, 34.26s/it]58it [22:11, 34.00s/it]59it [22:44, 33.72s/it]60it [23:20, 34.42s/it]61it [23:53, 34.02s/it]62it [24:28, 34.34s/it]63it [25:02, 34.16s/it]64it [25:38, 34.73s/it]65it [26:15, 35.27s/it]66it [26:48, 34.80s/it]67it [27:26, 35.50s/it]68it [28:01, 35.54s/it]69it [28:35, 34.91s/it]70it [29:09, 34.89s/it]71it [29:40, 33.56s/it]72it [30:13, 33.39s/it]73it [30:48, 33.88s/it]74it [31:20, 33.39s/it]75it [31:57, 34.27s/it]76it [32:31, 34.33s/it]77it [33:02, 33.25s/it]78it [33:37, 33.78s/it]79it [34:08, 33.01s/it]80it [34:39, 32.40s/it]81it [35:09, 31.78s/it]82it [35:42, 32.21s/it]83it [36:18, 33.25s/it]84it [36:57, 34.96s/it]85it [37:32, 35.06s/it]86it [38:07, 35.05s/it]87it [38:44, 35.57s/it]88it [39:21, 36.00s/it]89it [39:56, 35.65s/it]90it [40:27, 34.35s/it]91it [41:03, 34.66s/it]92it [41:39, 35.02s/it]93it [42:12, 34.45s/it]94it [42:46, 34.33s/it]95it [43:22, 34.86s/it]96it [43:56, 34.63s/it]97it [44:33, 35.25s/it]98it [45:08, 35.29s/it]99it [45:43, 35.25s/it]100it [46:18, 35.26s/it]101it [46:53, 34.93s/it]102it [47:27, 34.77s/it]103it [48:01, 34.51s/it]104it [48:34, 34.11s/it]105it [49:08, 34.09s/it]106it [49:44, 34.76s/it]107it [50:19, 34.78s/it]108it [50:53, 34.50s/it]109it [51:27, 34.19s/it]110it [52:03, 34.97s/it]111it [52:38, 34.92s/it]112it [53:10, 34.00s/it]113it [53:43, 33.77s/it]114it [54:16, 33.53s/it]115it [54:51, 33.79s/it]116it [55:24, 33.53s/it]117it [55:57, 33.58s/it]118it [56:33, 34.31s/it]119it [57:08, 34.29s/it]120it [57:46, 35.45s/it]121it [58:19, 34.66s/it]122it [58:55, 35.09s/it]123it [59:26, 33.96s/it]124it [1:00:01, 34.18s/it]125it [1:00:33, 33.51s/it]126it [1:01:07, 33.83s/it]127it [1:01:44, 34.59s/it]128it [1:02:19, 34.84s/it]129it [1:02:52, 34.19s/it]130it [1:03:27, 34.59s/it]131it [1:04:00, 34.22s/it]132it [1:04:35, 34.36s/it]133it [1:05:06, 33.16s/it]134it [1:05:40, 33.53s/it]135it [1:06:14, 33.55s/it]136it [1:06:49, 34.10s/it]137it [1:07:24, 34.41s/it]138it [1:07:58, 34.16s/it]139it [1:08:31, 34.02s/it]140it [1:09:06, 34.18s/it]141it [1:09:43, 34.96s/it]142it [1:10:28, 38.18s/it]143it [1:11:15, 40.72s/it]144it [1:12:04, 43.23s/it]145it [1:12:50, 44.09s/it]146it [1:13:38, 45.26s/it]147it [1:14:25, 45.65s/it]148it [1:15:11, 45.93s/it]149it [1:15:59, 46.49s/it]150it [1:16:47, 46.94s/it]151it [1:17:42, 49.22s/it]152it [1:18:36, 50.63s/it]153it [1:19:31, 52.03s/it]154it [1:20:32, 54.85s/it]155it [1:21:32, 56.36s/it]156it [1:22:32, 57.48s/it]157it [1:23:32, 58.08s/it]158it [1:24:32, 58.73s/it]159it [1:25:33, 59.49s/it]160it [1:26:33, 59.54s/it]161it [1:27:33, 59.60s/it]162it [1:28:33, 59.79s/it]163it [1:29:20, 56.01s/it]164it [1:30:08, 53.55s/it]165it [1:30:54, 51.37s/it]166it [1:31:40, 49.71s/it]167it [1:32:26, 48.74s/it]168it [1:33:14, 48.33s/it]169it [1:34:01, 47.98s/it]170it [1:34:47, 47.50s/it]171it [1:35:34, 47.23s/it]172it [1:36:20, 46.92s/it]173it [1:37:08, 47.07s/it]174it [1:37:54, 47.01s/it]175it [1:38:42, 47.26s/it]176it [1:39:29, 47.15s/it]177it [1:40:18, 47.53s/it]178it [1:41:04, 47.25s/it]179it [1:41:51, 46.96s/it]180it [1:42:38, 47.14s/it]181it [1:43:26, 47.26s/it]182it [1:44:12, 46.97s/it]183it [1:44:58, 46.59s/it]184it [1:45:45, 46.68s/it]185it [1:46:31, 46.57s/it]186it [1:47:18, 46.79s/it]187it [1:48:04, 46.42s/it]188it [1:48:52, 46.96s/it]189it [1:49:38, 46.76s/it]190it [1:50:25, 46.67s/it]191it [1:51:11, 46.55s/it]192it [1:51:58, 46.74s/it]193it [1:52:44, 46.60s/it]194it [1:53:17, 42.52s/it]195it [1:53:52, 40.11s/it]196it [1:54:26, 38.30s/it]197it [1:55:00, 36.96s/it]198it [1:55:32, 35.51s/it]199it [1:56:04, 34.50s/it]200it [1:56:36, 33.73s/it]201it [1:57:08, 33.22s/it]202it [1:57:41, 33.13s/it]203it [1:58:14, 33.04s/it]204it [1:58:44, 32.18s/it]205it [1:59:15, 31.85s/it]206it [1:59:46, 31.57s/it]207it [2:00:19, 32.09s/it]208it [2:00:52, 32.38s/it]209it [2:01:26, 32.64s/it]210it [2:01:58, 32.64s/it]211it [2:02:32, 32.96s/it]212it [2:03:00, 31.64s/it]213it [2:03:34, 32.21s/it]214it [2:04:04, 31.67s/it]215it [2:04:36, 31.66s/it]216it [2:05:09, 31.92s/it]217it [2:05:42, 32.39s/it]218it [2:06:16, 32.82s/it]219it [2:06:49, 32.98s/it]220it [2:07:21, 32.49s/it]221it [2:07:52, 32.16s/it]222it [2:08:24, 32.17s/it]223it [2:08:57, 32.36s/it]224it [2:09:30, 32.72s/it]225it [2:10:02, 32.33s/it]226it [2:10:34, 32.35s/it]227it [2:11:07, 32.34s/it]228it [2:11:39, 32.27s/it]229it [2:12:12, 32.69s/it]230it [2:12:46, 32.81s/it]231it [2:13:17, 32.33s/it]232it [2:13:48, 32.06s/it]233it [2:14:21, 32.39s/it]234it [2:14:54, 32.45s/it]235it [2:15:27, 32.68s/it]236it [2:15:57, 31.98s/it]237it [2:16:31, 32.60s/it]238it [2:16:59, 30.99s/it]239it [2:17:32, 31.73s/it]240it [2:18:04, 31.88s/it]241it [2:18:39, 32.66s/it]242it [2:19:11, 32.57s/it]243it [2:19:46, 33.29s/it]244it [2:20:18, 32.93s/it]245it [2:20:52, 33.04s/it]246it [2:21:22, 32.29s/it]247it [2:21:53, 31.97s/it]248it [2:22:24, 31.66s/it]249it [2:22:56, 31.69s/it]250it [2:23:25, 30.74s/it]251it [2:23:53, 30.05s/it]252it [2:24:24, 30.47s/it]253it [2:24:57, 31.02s/it]254it [2:25:28, 31.00s/it]255it [2:26:02, 31.88s/it]256it [2:26:35, 32.35s/it]257it [2:27:08, 32.37s/it]258it [2:27:39, 32.16s/it]259it [2:28:13, 32.65s/it]260it [2:28:43, 31.92s/it]261it [2:29:16, 32.14s/it]262it [2:29:46, 31.44s/it]263it [2:30:20, 32.16s/it]264it [2:30:51, 32.03s/it]265it [2:31:24, 32.31s/it]266it [2:31:58, 32.69s/it]267it [2:32:30, 32.59s/it]268it [2:33:03, 32.58s/it]269it [2:33:31, 31.15s/it]270it [2:34:00, 30.62s/it]271it [2:34:31, 30.88s/it]272it [2:35:03, 31.09s/it]273it [2:35:35, 31.29s/it]274it [2:36:06, 31.37s/it]275it [2:36:42, 32.64s/it]276it [2:37:16, 33.23s/it]277it [2:37:50, 33.36s/it]278it [2:38:23, 33.32s/it]279it [2:38:57, 33.28s/it]280it [2:39:30, 33.44s/it]281it [2:40:02, 32.96s/it]282it [2:40:35, 32.98s/it]283it [2:41:08, 32.97s/it]284it [2:41:41, 32.86s/it]285it [2:41:58, 28.11s/it]286it [2:42:14, 24.65s/it]287it [2:42:31, 22.17s/it]288it [2:42:47, 20.45s/it]289it [2:43:05, 19.73s/it]290it [2:43:23, 19.21s/it]291it [2:43:42, 18.95s/it]292it [2:44:00, 18.84s/it]293it [2:44:19, 18.80s/it]294it [2:44:37, 18.58s/it]295it [2:44:56, 18.75s/it]296it [2:45:14, 18.60s/it]297it [2:45:33, 18.55s/it]298it [2:45:50, 18.11s/it]299it [2:46:07, 17.83s/it]300it [2:46:24, 17.63s/it]301it [2:46:42, 17.53s/it]302it [2:46:59, 17.41s/it]303it [2:47:16, 17.26s/it]304it [2:47:33, 17.24s/it]305it [2:47:50, 17.14s/it]306it [2:48:07, 17.22s/it]307it [2:48:23, 16.97s/it]308it [2:48:40, 16.97s/it]309it [2:48:57, 16.95s/it]310it [2:49:14, 16.99s/it]311it [2:49:32, 17.09s/it]312it [2:49:49, 17.09s/it]313it [2:50:06, 16.99s/it]313it [2:50:06, 32.61s/it]
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: - 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: \ 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: - 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: \ 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb: mean_reward ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: mean_reward 1.72237
wandb: 
wandb: Synced decent-totem-21: https://wandb.ai/dahoas/trl_accelerate/runs/12ipiqfk
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220915_203348-12ipiqfk/logs
Traceback (most recent call last):
  File "test_trl.py", line 161, in <module>
    output = gpt2_model_ref.generate(torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device),
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/generation_utils.py", line 1326, in generate
    return self.sample(
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/generation_utils.py", line 1942, in sample
    outputs = self(
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/alex/repos/trl/trl/gpt2.py", line 109, in forward
    transformer_outputs = self.transformer(
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 844, in forward
    inputs_embeds = self.wte(input_ids)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/torch/nn/modules/sparse.py", line 158, in forward
    return F.embedding(
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/torch/nn/functional.py", line 2199, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)
